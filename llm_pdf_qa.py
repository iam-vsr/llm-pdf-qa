# -*- coding: utf-8 -*-
"""llm-pdf-qa.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CjeS-PdTSxa2d28L6DQ9kI2NSvCqAweS

# üîç **LLM-Powered PDF Question Answering System**

This project demonstrates how to build a Question Answering (QA) system over PDF documents using Large Language Models (LLMs).  
Users can upload a PDF, ask questions about its content, and receive accurate, context-based answers.

**Key Technologies**: LangChain, FAISS, HuggingFace, Gradio, PyTorch

## **Making Preparations**

***Make a utils directory***
"""

!mkdir -p utils

"""***Download and Upload config.json and custom_logger.py files from this [Github Repo](https://github.com/iam-vsr/llm-pdf-qa)***"""

from google.colab import files
uploaded = files.upload()

"""***Code to move custom_logger.py to utils folder created earlier***"""

import shutil
shutil.move('custom_logger.py', 'utils/custom_logger.py')

"""***Install Required Packages***"""

!pip install gradio langchain accelerate sentence_transformers pypdf tiktoken bitsandbytes

!pip install faiss-cpu

import faiss
print(f"FAISS version: {faiss.__version__}")

!pip install -U langchain-community

"""***Import Libraries & Load Config***"""

from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import TokenTextSplitter
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
import pickle
import os
import gradio as gr
import json
import re

from utils.custom_logger import CustomLogger
logger = CustomLogger()

from langchain import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
import torch
from utils.custom_logger import logger

"""##**Document Processing & Embedding**

This section includes:
- Loading PDF pages
- Splitting documents into chunks
- Creating and storing document embeddings
- Loading embeddings if already available

"""

class DataLoadPDF:

    """
    A class for loading data from a PDF file.
    """

    def __init__(self, file_path):

        """
        Initialize the DataLoadPDF instance.
        Args:
            file_path (str): Path to the PDF file to load.
        """

        self.file_path = file_path

    def load_data(self):

        """
        Load data from the PDF file.=
        Returns:
            list: List of pages from the PDF.
        """

        logger.info(f"Reading file {os.path.basename(self.file_path)} ... ")
        loader = PyPDFLoader(self.file_path)
        pages = loader.load()

        return pages

class DataSplitter:

    """
    A class for splitting data into chunks.
    """

    def __init__(self, chunk_size, chunk_overlap):

        """
        Initialize the DataSplitter instance.
        Args:
            chunk_size (int): Size of each chunk.
            chunk_overlap (int): Overlap between consecutive chunks.
        """

        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

    def split_data(self, pages):

        """
        Split data into chunks.

        Args:
            pages (list): List of data pages.
        Returns:
            list: List of split documents.
        """

        logger.info(f"Document splitting with chunk_size {self.chunk_size} and chunk_overlap {self.chunk_overlap} ... ")

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=["\n\n", "\n", ".", " ", ""]
            )

        docs = text_splitter.split_documents(pages)
        return docs

class EmbeddingManager:

    """
    A class for managing document embeddings.
    """

    def __init__(self, model_name):

        """
        Initialize the EmbeddingManager instance.

        Args:
            model_name (str): Name of the embedding model.
        """

        self.model_name = model_name
        logger.info(f"Loading embeddings Model {self.model_name} ... ")
        self.embeddings = HuggingFaceEmbeddings(model_name=self.model_name)

    def create_embeddings(self, docs):

        """
        Create embeddings for documents.

        Args:
            docs (list): List of documents.
        Returns:
            FAISS: Document embeddings.
        """

        logger.info(f"Creating document embeddings for {len(docs)} split ... ")
        self.doc_embedding = FAISS.from_documents(docs, self.embeddings)
        return self.doc_embedding

    def save_embedding(self, file_name):

        """
        Save document embeddings to a file.

        Args:
            file_name (str): Name of the file to save the embeddings.
        """

        emedding_dir = "embeddings_data"

        if not os.path.exists(emedding_dir):
            os.mkdir(emedding_dir)

        file_name = os.path.basename(file_name)
        logger.info(f"Saving document embeddings: {'embeddings_data/'+file_name} ... ")

        with open("embeddings_data/"+file_name+".pkl", "wb") as f:
            pickle.dump(self.doc_embedding, f)

    def load_embedding(self, file_name):

        """
        Load document embeddings from a file.

        Args:
            file_name (str): Name of the file to load the embeddings.
        Returns:
            FAISS: Loaded document embeddings.
        """

        file_name = os.path.basename(file_name)
        logger.info(f"Loading document embeddings locally: {'embeddings_data/'+file_name} ... ")

        with open("embeddings_data/"+file_name+".pkl", "rb") as f:
            self.doc_embedding = pickle.load(f)

        return self.doc_embedding

    def check_embedding_available(self, file_name):

        """
        Check if document embeddings are available in a file.
        Args:
            file_name (str): Name of the file to check.
        Returns:
            bool: True if document embeddings are available, False otherwise.
        """

        file_name = os.path.basename(file_name)
        doc_check = os.path.isfile("embeddings_data/"+file_name+".pkl")
        logger.info(f"Is document embedding found: {doc_check}")

        return doc_check

class DocumentProcessor:

    """
    A class for processing documents and managing embeddings.
    """
    def __init__(self, model_name, chunk_size, chunk_overlap):

        """
        Initialize the DocumentProcessor instance.

        Args:
            model_name (str): Name of the embedding model.
            chunk_size (int): Size of each chunk.
            chunk_overlap (int): Overlap between consecutive chunks.
        """

        logger.info(f"Initializing document processor parameters - embedding model_name: {model_name}, chunk_size: {chunk_size}, chunk_overlap: {chunk_overlap} ... ")

        self.model_name = model_name
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.embedding_manager = EmbeddingManager(model_name)

    def process_document(self, file_path):

        """
        Process a document and manage embeddings.
        Args:
            file_path (str): Path to the document file.
        Returns:
            FAISS: Document embeddings.
        """

        if self.embedding_manager.check_embedding_available(file_path):
            return self.embedding_manager.load_embedding(file_path)

        else:
            data_loader = DataLoadPDF(file_path)
            pages = data_loader.load_data()
            data_splitter = DataSplitter(self.chunk_size, self.chunk_overlap)
            docs = data_splitter.split_data(pages)
            doc_embedding = self.embedding_manager.create_embeddings(docs)

            self.embedding_manager.save_embedding(file_path)

            return doc_embedding

"""##**Loading the LLM Model**

Used `meta-llama/Llama-2-7b-chat-hf` from Hugging Face, loaded using Transformers pipeline.  
Supports int8 loading for memory efficiency.

"""

class ModelLoader:
    """
    A class responsible for loading the language model.
    """
    def __init__(self, model_id, max_length, temperature,load_int8):
        """
        Initialize the ModelLoader instance.
        Args:
            model_id (str): Identifier of the pretrained model.
            max_length (int): Maximum length of generated text.
            temperature (float): Temperature parameter for text generation.
        """
        self.model_id = model_id
        self.max_length = max_length
        self.temperature = temperature
        self.load_int8 = load_int8

    def load_model(self):
        """
        Load the language model using the specified model_id, max_length, and temperature.

        Returns:
            HuggingFacePipeline: Loaded language model.
        """
        logger.info(f"Loading LLM model {self.model_id} with max_length {self.max_length} and temperature {self.temperature}...\n")
        tokenizer = AutoTokenizer.from_pretrained(self.model_id)
        if self.load_int8:
            model = AutoModelForCausalLM.from_pretrained(self.model_id, load_in_8bit=True, device_map="auto")
        else:
            model = AutoModelForCausalLM.from_pretrained(self.model_id, torch_dtype=torch.bfloat16, device_map="auto")

        logger.info("Model is loaded successfully\n")
        pipe = pipeline(
            "text-generation", model=model, tokenizer=tokenizer, max_length=self.max_length, temperature=self.temperature
        )
        llm = HuggingFacePipeline(pipeline=pipe)
        return llm

"""##**Setting up the Retrieval-Based QA System**

- Uses LangChain‚Äôs `RetrievalQA` chain
- Combines context retrieval and answer generation
- Prompt template is customizable

"""

class QASystem:

    """
    A class representing a Question Answering (QA) system.
    """
    def __init__(self, llm):

        """
        Initialize the QASystem instance.
        Args:
            llm (HuggingFacePipeline): Loaded language model for text generation.
        """

        self.llm = llm

        self.prompt_template = """You are a helpful and concise assistant.Answer the question as best as you can.
        If you cannot find an answer, say "I don't know".
        Context:{context}
        Question: {question}
        Answer (based on the context above):"""

        PROMPT = PromptTemplate(
            template=self.prompt_template, input_variables=["context", "question"]
        )

        self.chain_type_kwargs = {
            "prompt": PROMPT,
        }

    def setup_retrieval_qa(self, doc_embedding):

        """
        Set up the retrieval-based QA system.
        Args:
            doc_embedding: Document embedding for retrieval.
        Returns:
            RetrievalQA: Configured retrieval-based QA system.
        """

        logger.info("Setting up retrieval QA system...\n")

        qa = RetrievalQA.from_chain_type(

            llm=self.llm,
            chain_type="stuff",  # You might need to replace this with the appropriate chain type.
            retriever = doc_embedding.as_retriever(
                search_type="similarity_score_threshold",
                search_kwargs={"score_threshold": 0.5, "k": 6}
                ),
            chain_type_kwargs=self.chain_type_kwargs,
            )

        return qa

"""## **Adding Hugging Face token**"""

from huggingface_hub import login
from getpass import getpass

hf_token = getpass("Enter your Hugging Face token:")
login(token=hf_token)

"""## **Loading model and processing parameters from config.json for flexible and centralized configuration management**"""

with open('config.json', 'r') as config_file:
    config = json.load(config_file)

logger.info(f"Loaded config file: {config}")

"""## **Initialize the embedding-based document processor and load the LLM based on parameters from the config.**"""

# Loading embedding model
document_processor = DocumentProcessor(model_name=config["embedding_model_name"], chunk_size=config["chunk_size"], chunk_overlap=config["chunk_overlap"])

# Load model globally
model_loder = ModelLoader(config["model_id"], config["max_length"], config["temperature"],config['load_int8'])

llm = model_loder.load_model()

qa_system = QASystem(llm)

# Initialize global variable for doc_embedding
doc_embedding = None
pdf_file_name = None
qa = None

"""##**Defining Chatbot Logic**

This function:
- Handles file change detection
- Loads or creates document embeddings
- Performs retrieval + LLM-based answering

"""

def chatbot(pdf_file,query):

    global doc_embedding
    global pdf_file_name
    global qa

    if pdf_file_name is None or pdf_file_name!= pdf_file.name or doc_embedding is None:
        logger.info("New PDF Found Resetting doc_embedding")
        doc_embedding = None
        pdf_file_name = pdf_file.name

    if doc_embedding is None:
        logger.info("Starting for new doc_embedding")
        doc_embedding = document_processor.process_document(pdf_file.name)
        qa = qa_system.setup_retrieval_qa(doc_embedding)

    result = qa.invoke({"query": query})

    def remove_duplicate_lines(text):
      seen = set()
      result = []
      for line in text.split("\n"):
        line = line.strip()
        if line and line not in seen:
          seen.add(line)
          result.append(line)
      return "\n".join(result)

    return remove_duplicate_lines(result['result'])

"""##**Building the User Interface with Gradio**

The Gradio app allows:
- PDF Upload
- User Query Input
- Real-Time Answers from LLM

Launches with `share=True` for public demo.

***I'll be using a Competitive Programming Handbook PDF as input, you can found it on my [GitHub Repo](https://github.com/iam-vsr/llm-pdf-qa)***
"""

with gr.Blocks(theme=gr.themes.Default(primary_hue="red", secondary_hue="pink")) as demo:
    gr.Markdown("# Ask your Question to PDF Document")

    with gr.Row():
        with gr.Column(scale=4):
            pdf_file = gr.File(label="Upload your PDF")

    output = gr.Textbox(label="output",lines=3)
    query = gr.Textbox(label="query")
    btn = gr.Button("Submit")
    btn.click(fn=chatbot, inputs=[pdf_file,query], outputs=[output])

gr.close_all()
demo.launch(share=True, debug=True)

!jupyter nbconvert --to notebook --ClearOutputPreprocessor.enabled=True --inplace your_notebook.ipynb

